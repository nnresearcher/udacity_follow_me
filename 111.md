
[image1]: ./images/graph1.png
[image2]: ./images/graph2.png
[image3]: ./images/graph3.png
[image4]: ./images/graph4.png
[image5]: ./images/predict1.png
[image6]: ./images/predict2.png
[image7]: ./images/prediction3.png
[image8]: ./images/predict4.png
[image9]: ./images/predict5.png
[image10]: ./images/predict6.png

## Project Overview
This project aims at writing codes that will make a Robot capable of identifying pixels in a target image and able to follow that target as it moves in the simulated world. This is can be achieved using Deep Learning segmentation which does not just end at image classifications but goes further to perform pixels identification in an image.

## Initial requirements for the Implementation
At the beginning of this Implementation, I had to build an encoder and decoder

### The encoder role
The encoder ensures a convolution network that reduces to a deeper 1x1 convolution layer, in contrast to a flat fully connected layer that would be used for basic classification of images(pixel wise) in this case the moving target in the world.
The encoder can be enforced by means a technique called `Separable Convolutions`. which increases the efficiency of the network by reducing the number of parameter needed by the network to achieve a successful encoder.


### The decoder role
The decoder as the name implies just reverses the role of the encoder. It is achieved by a technique called `Bilinear Upsampling` which makes use of weighted average of four nearest known pixels, located diagonally to a given pixel, to estimate a new pixel intensity value.
Code wise it can be achieved by use of the `utils.separable_conv2d` function in keras

The above networks are Implemented in the `encoder_block` and `decoder_block` methods of the `model_train`.ipynb file.

It should be noted that the encoder block makes use of a stride of 2 which a measure of how much a filter is shifted on an image with each step. As stride of 2 was chosen so that the shape of the network layer matches with that of the decoder. If this value has to be changed then an appropriate change has to be enforced on the network layer prior to the decoder.

## The network architecture
With the encoder and decoder functionality, I was not able to build the model architecture for this Implementation. This phase comprised of 3 steps.

### Addition of encoder blocks
The first step of this phase requires the conjunction of a number of encoder blocks. These blocks are required to provide a number of separable convolution layers which will be used to to form a 1x1 convolution layer. It should be noted that in this step the depth of the model of the number of filters from each block to the next increases and a general rule of thumb, the filter size should be twice the size of the preceding encoder block.
Note that `filters` play an Important role in finding key features in an image.  A larger size filter can overlook at the features and could skip the essential details in the images whereas a smaller size filter could provide more information leading to more confusion.

### Addition of  1x1 Convolution layer
Another required layer for this phase is the addition of 1x1 convolution layer which simply maps input pixel to an output pixel without looking at anything in itself. It is often used to reduce the number of depth channels, since it is often very slow to multiply volumes with extremely large depths.
In this case I build a 1x1 Convolution layer from the last encoder block and filter we created in the previous phase above. Also I used a kernel size of 1.

### Addition of decoder blocks
Here we add a number o decoder blocks matching the number of encoder blocks that were previously added. These decoders are constructed from the encoder blocks already added in the first step of this phase with a decreasing depth from one decoder block to the next.

The last decoder block is then used to return the final output layer by passing it through the `tf.contrib.keras.layers.Conv2D` method to get the a convolved layer using a `softmax` activation function with `same` padding.

## Training
After having a convolved layer as an output layer from our model architecture, it was time to move over to Training.
In this phase I had to play around with the various hyperparameters. The various hyperparameters I considered are;

  - `batch_size`: number of training samples/images that get propagated through the network in a single pass. I decided to set this value to `16` and this number was randomly selected since to the best of this training delivered the best reflection on the final result of this Implementation. Also a large number as batch_size will just make the network rush over the learning process and also a too small value will allow for cramming and also to an extent make the training phase slower.

  - `num_epochs`: number of times the entire training dataset gets propagated through the network. This value shoul be choosen such that we avoid overfitting. I had to test various values such that in each iteration the network learns considerably and i found this number to be `6`.

  - `steps_per_epoch`: number of batches of training images that go through the network in 1 epoch. Its recommended to divide the total number of images in the training set by the batch_size. In this came I got the value 170.

  - `validation_steps`: number of batches of validation images that go through the network in 1 epoch. For best performance I landed on the value `50`.

  - `workers`: maximum number of processes to spin up. I tried a couple of values and by try and error I landed on `2` as this showed to present better performance.

  Below shows the behavior our the trained model as it kept learning.

  ![alt text][image1]
  ![alt text][image2]
  ![alt text][image3]
  ![alt text][image4]


  With the hyperparameters decided upon, our model as now ready to be trained. Our trained model then underwent prediction for three instances as described below.

    - Prediction in cases where the quadcopter follows the target from behind:

  ![alt text][image5]

    - Prediction in cases where the quadcopter is patroling without target:

  ![alt text][image7]

    - Prediction in cases where the quadcopter is patroling with target:

  ![alt text][image9]

  Finally I performed a validation step on the trained model and got various scores for various instances among which I will love to emphasize on the `final_IoU` and `final_score` values which stands out to be `0.492944565523` and `0.349653159832` respectively. It should be noted that these values were gotten in conformity with the values for hyperparameters mentioned above.
